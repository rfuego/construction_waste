# Stable Diffusion Application using React with FastAPI


Hello, I would like to share with you this small project that involves creating an Stable Diffusion App with React and FastAPI :grin:. 


Let's dive in by introducing what is the project about and why I have done it. 

## 1. What is this project about? 

Команда Fuego представляет модель и интерфейс для автоматического распознавания видов отходов строительства и сноса. В качестве решения использована модель YOLOv8, обученная на датасете, собранном в сервисе Roboflow, и дополнительно дообученная на данных, которые получены с камер видеонаблюдения.

Разработанное решение позволяет классифицировать отходы строительного мусора и сноса в кузове спецтехники, и предотвратить подмену заявленного типа, который указан при запуске машины в рейс.

## 2. Why I did built it?

For the past couple of months I've been hearing a lot about Stable Diffusion but I haven't had the time to explore it myself, then I saw an opportunity to get to know it but also do a refresh on my frontend skills which were a bit out-of-date by building a React App. 

Additionally, I wanted to learn more about FastAPI and this project is built on top of the webinar provided by the author of the library [Sebastian Ramirez](https://github.com/tiangolo). 

## 3. Concepts 

Before diving into the project, I'll give you an overview of the main concepts and technologies that have been used, and pointers to resources where you can learn more about it. 

### 3.1 React as Front-end 

React как Front-end 

Возможно, вы уже слышали о React, поэтому можете пропустить этот раздел, если это не краткое изложение о React. 

[[React](https://reactos.org/) - библиотека JavaScript для создания пользовательского интерфейса. Он был создан Facebook (он же Meta) примерно в 2013 году. 

Насколько я смог узнать во время работы над этим проектом, React работает декларативно, и одна из особенностей, которая делает его классным, заключается в том, что вы можете удалять компоненты, которые имеют свое собственное состояние, и позже могут быть повторно использованы в приложении, которое вы создаете. 

Исходя из чисто серверного опыта, я могу сказать, что было не так уж сложно получить и включить его в то, чего я хотел достичь, а именно создать простой пользовательский интерфейс, который позволит мне писать текст и ждать, пока сервер сгенерирует изображение.


[React](https://reactjs.org/) is a JavaScript library for building UI. 
#### 3.2.1 Resources
- [React Docs](https://reactjs.org/docs/getting-started.html)

### 3.3 FastAPI as backend

In here, things become more interesting to me when backend comes in and I discovered all you can do working with FastAPI. 

As their website state: 
> FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.

What I like from FastAPI is the ability to create API's quickly without too much hassle. I loved the fact I could define the routes and immediately check them out by looking at the API docs provided by Swaggeer. 

Additionally to this, you could define your data model as a Class by using pydantic inheriting from BaseModel. 

#### 3.3.1 Resources
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [FastAPI Tutorial - User Guide](https://fastapi.tiangolo.com/tutorial/)

## Project Structure

```
├── backend                          ----> Contains all the backend FastAPI work
│   ├── dev-requirements.txt
│   ├── main.py                      ----> Endpoints definition
│   ├── requirements.txt
│   ├── run_uvicorn.py
│   ├── schemas.py                   ----> Define your data models here
│   └── services.py                  ----> Define all the heavy load work to be done here.
|                                          in this case all the HuggingFace setup and work for generating the 
|                                          stable diffusion image
└── frontend
    ├── README.md
    ├── package-lock.json
    ├── package.json
    ├── public
    │   ├── index.html
    │   ├── manifest.json
    │   └── robots.txt
    └── src
        ├── App.jsx                 ---> Main App definition where you will embed your components as well
        ├── components
        │   ├── ErrorMessage.jsx
        │   ├── GenImage.jsx        ---> Definition of the UI components as well as the call to the backend 
                                         using fetch API
        │   └── Header.jsx          ---> Minimal Header definition
        └── index.js
```

## Setup and run it :) 

These are the steps to see it running :) 

### From your backend folder: 

1. You need a HugginFace token. Checkout how to create one [here](https://huggingface.co/docs/hub/security-tokens#how-to-manage-user-access-tokens)
2. Once you have your token created it follow the next steps

```
cd backend 
touch .env 
```

3. Open the .env file and add your token there like this: 

```
HF_TOKEN=MY_FANCY_TOKEN
```

**WARNING**: Make sure you never commit your keys or tokens file :) Add this file to your .gitignore

4. Create your environment and activate your environment
```
python -m venv venv 
source venv/bin/activate
pip install -r requirements.txt
```

5. Startup your backend 

```
uvicorn main:app --port 8885
```

### From your frontend folder

```
cd frontend
npm install 
npm start
```

## How to Generate an Image? 

Fill the parameters as follows:

- Prompt: Text to express the wish for your image. 

    `Example: A red racing car winning the formula-1` 
- Seed: a number indicating the seed so that your output will be deterministic. 
- Guidance scale: it's a float that basically try to enforce that the generation of the image better match the prompt. 

    > Side note: if you are curious about this parameter, you can do a deep dive by reading the paper [Classifier-free Diffusion Guidance](https://arxiv.org/pdf/2207.12598.pdf)
- Number of Inference Steps: It's a number usually between 50 and 100. This number indicates de amount of denoising steps. Keep in mind the higher the number the more time the inference ( image generation ) will take. 

You should be able to see your frontend like this one below: 

<p align="center">
<img src="./assets/demo.png" alt="Demo example" width=700 height=400 />
</p>

## Resources I've used to build the project

- Webinar: [FastAPI for Machine Learning: Live coding an ML web application](https://www.youtube.com/watch?v=_BZGtifh_gw)
- FastAPI and ReactJS, a set of videos by [Rithmic - Youtube Channel](https://youtube.com/playlist?list=PLhH3UpV2flrwfJ2aSwn8MkCKz9VzO-1P4)
- [Stable Diffusion with 🧨 Diffusers by HuggingFace](https://huggingface.co/blog/stable_diffusion)

## ToDo

- Add endpoint that shows a grid of images instead of one
- Update the service backend with a new function to return multiple images. 
- Improve the UI by customizing Bulma with a different style. 
